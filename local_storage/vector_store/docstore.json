{"docstore/metadata": {"93b3519e-edc7-4b30-a0f7-74325161e4a0": {"doc_hash": "0adec09e4300379c83ef41d5f401a97485cf929744f050a6cbfec32db8458638"}, "7e6eb05b-9a3e-431d-8453-3c32c00a27cc": {"doc_hash": "91d5fe64e0f07acf92a09b93c997f642b734c311027d45f79a7a48dd671c1b49", "ref_doc_id": "93b3519e-edc7-4b30-a0f7-74325161e4a0"}, "40874622-2ab8-4199-98ba-fd837ac51a75": {"doc_hash": "f4cbc3045f8e71c91ce20fc2279bcc1cd54876dbc6ac1f7d095e6df20a0084e6", "ref_doc_id": "93b3519e-edc7-4b30-a0f7-74325161e4a0"}, "e5c71aa9-0df5-407a-9568-8a81757ad8a3": {"doc_hash": "8623215397082b722e482b7833bfeeb42de909780dfc410ec90e42ec77d3ef98", "ref_doc_id": "93b3519e-edc7-4b30-a0f7-74325161e4a0"}, "68cad23f-3a43-41d6-bbe9-4fb44897adce": {"doc_hash": "a18565ac4dbbd0f4fd651e9669d50e6475025957534a11bacc50b0a4ccc6fa4a", "ref_doc_id": "93b3519e-edc7-4b30-a0f7-74325161e4a0"}, "12a1ff97-e515-4c1c-9e15-7e211bb44772": {"doc_hash": "69bd3bcbd83263970ec64c439d0019208a4fe3e4bfd06dff848816f606c299a0", "ref_doc_id": "93b3519e-edc7-4b30-a0f7-74325161e4a0"}}, "docstore/ref_doc_info": {"93b3519e-edc7-4b30-a0f7-74325161e4a0": {"node_ids": ["7e6eb05b-9a3e-431d-8453-3c32c00a27cc", "40874622-2ab8-4199-98ba-fd837ac51a75", "e5c71aa9-0df5-407a-9568-8a81757ad8a3", "68cad23f-3a43-41d6-bbe9-4fb44897adce", "12a1ff97-e515-4c1c-9e15-7e211bb44772"], "metadata": {"file_path": "/Users/hemagunasekaran/Desktop/DATA SCIENCE & AI/rag_project/data/scikit_learn_notes.txt", "file_name": "scikit_learn_notes.txt", "file_type": "text/plain", "file_size": 8338, "creation_date": "2025-09-29", "last_modified_date": "2025-09-29"}}}, "docstore/data": {"7e6eb05b-9a3e-431d-8453-3c32c00a27cc": {"__data__": {"id_": "7e6eb05b-9a3e-431d-8453-3c32c00a27cc", "embedding": null, "metadata": {"file_path": "/Users/hemagunasekaran/Desktop/DATA SCIENCE & AI/rag_project/data/scikit_learn_notes.txt", "file_name": "scikit_learn_notes.txt", "file_type": "text/plain", "file_size": 8338, "creation_date": "2025-09-29", "last_modified_date": "2025-09-29"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "93b3519e-edc7-4b30-a0f7-74325161e4a0", "node_type": "4", "metadata": {"file_path": "/Users/hemagunasekaran/Desktop/DATA SCIENCE & AI/rag_project/data/scikit_learn_notes.txt", "file_name": "scikit_learn_notes.txt", "file_type": "text/plain", "file_size": 8338, "creation_date": "2025-09-29", "last_modified_date": "2025-09-29"}, "hash": "0adec09e4300379c83ef41d5f401a97485cf929744f050a6cbfec32db8458638", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "40874622-2ab8-4199-98ba-fd837ac51a75", "node_type": "1", "metadata": {}, "hash": "0dbe400cdd52bad89ebd29bc43f6291fb0bc06d3a645844eec9d1fe322115479", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "=============================\nSCIKIT-LEARN CHEATSHEET NOTES\n=============================\n\n\u2022 Pandas: Data manipulation and analysis. Functions: df.head(), df.info(), df.groupby(), df.merge().\n\u2022 Numpy: Numerical computing with arrays. Functions: np.array(), np.mean(), np.std(), np.dot().\n\u2022 Train-test split: from sklearn.model_selection import train_test_split\n\u2022 Cross-validation: from sklearn.model_selection import cross_val_score, KFold, StratifiedKFold\n\u2022 Pipelines: from sklearn.pipeline import Pipeline \u2013 chains preprocessing + model steps\n\u2022 Scaling: StandardScaler, MinMaxScaler, RobustScaler\n\u2022 Feature selection: SelectKBest, RFE\n\u2022 Dimensionality reduction: PCA, TruncatedSVD\n\u2022 Classification: LogisticRegression, KNeighborsClassifier, DecisionTreeClassifier, RandomForestClassifier, SVC\n\u2022 Regression: LinearRegression, Ridge, Lasso, DecisionTreeRegressor, RandomForestRegressor\n\u2022 Clustering: KMeans, DBSCAN, AgglomerativeClustering\n\u2022 Metrics:\n    \u2013 Classification: accuracy_score, precision_score, recall_score, f1_score, roc_auc_score\n    \u2013 Regression: mean_squared_error, mean_absolute_error, r2_score\n    \u2013 Clustering: silhouette_score, adjusted_rand_score\n\u2022 Bias-variance trade-off: Balance between underfitting and overfitting\n\u2022 Model persistence: joblib.dump(), joblib.load()\n\u2022 Hyperparameter tuning: GridSearchCV, RandomizedSearchCV\n\n=========================\nEXAM-STYLE Q&A PRACTICE\n=========================\n\nQ: What is PCA and why do we use it?\nA: PCA reduces dimensionality by projecting features into orthogonal principal components, preserving maximum variance. Used to remove redundancy, reduce noise, and speed up ML models.\n\nQ: How does KMeans clustering work?\nA: KMeans initializes k centroids, assigns points to nearest centroid, updates centroids as means, and iterates until convergence. It minimizes within-cluster variance.\n\nQ: Explain the bias-variance trade-off.\nA: Bias = underfitting, overly simple model. Variance = overfitting, overly complex model. The trade-off is balancing simplicity vs complexity for best generalization.\n\nQ: What is train-test split and why is it used?\nA: It divides data into training and testing sets to evaluate generalization on unseen data.", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 2204, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "40874622-2ab8-4199-98ba-fd837ac51a75": {"__data__": {"id_": "40874622-2ab8-4199-98ba-fd837ac51a75", "embedding": null, "metadata": {"file_path": "/Users/hemagunasekaran/Desktop/DATA SCIENCE & AI/rag_project/data/scikit_learn_notes.txt", "file_name": "scikit_learn_notes.txt", "file_type": "text/plain", "file_size": 8338, "creation_date": "2025-09-29", "last_modified_date": "2025-09-29"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "93b3519e-edc7-4b30-a0f7-74325161e4a0", "node_type": "4", "metadata": {"file_path": "/Users/hemagunasekaran/Desktop/DATA SCIENCE & AI/rag_project/data/scikit_learn_notes.txt", "file_name": "scikit_learn_notes.txt", "file_type": "text/plain", "file_size": 8338, "creation_date": "2025-09-29", "last_modified_date": "2025-09-29"}, "hash": "0adec09e4300379c83ef41d5f401a97485cf929744f050a6cbfec32db8458638", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "7e6eb05b-9a3e-431d-8453-3c32c00a27cc", "node_type": "1", "metadata": {"file_path": "/Users/hemagunasekaran/Desktop/DATA SCIENCE & AI/rag_project/data/scikit_learn_notes.txt", "file_name": "scikit_learn_notes.txt", "file_type": "text/plain", "file_size": 8338, "creation_date": "2025-09-29", "last_modified_date": "2025-09-29"}, "hash": "91d5fe64e0f07acf92a09b93c997f642b734c311027d45f79a7a48dd671c1b49", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "e5c71aa9-0df5-407a-9568-8a81757ad8a3", "node_type": "1", "metadata": {}, "hash": "c13f4ebd4a64316e2795072f8ffe10da9cc3dc4e2bca857efcba7e8d1fa76b59", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Q: Explain the bias-variance trade-off.\nA: Bias = underfitting, overly simple model. Variance = overfitting, overly complex model. The trade-off is balancing simplicity vs complexity for best generalization.\n\nQ: What is train-test split and why is it used?\nA: It divides data into training and testing sets to evaluate generalization on unseen data.\n\nQ: What is cross-validation?\nA: A method to evaluate models more reliably by splitting data into folds and averaging performance.\n\nQ: What is a pipeline in scikit-learn?\nA: An object that chains preprocessing steps (scaling, encoding) with a model for consistency and simplicity.\n\nQ: Difference between StandardScaler, MinMaxScaler, and RobustScaler?\nA: StandardScaler: mean=0, std=1. MinMaxScaler: rescales to [0,1]. RobustScaler: uses median/IQR, robust to outliers.\n\nQ: Supervised vs unsupervised learning?\nA: Supervised: learns from labeled data (classification, regression). Unsupervised: finds structure in unlabeled data (clustering, PCA).\n\nQ: How does KNN work?\nA: Classifies a point based on the majority label of k nearest neighbors using a distance metric.\n\nQ: LogisticRegression vs LinearRegression?\nA: LinearRegression predicts continuous values. LogisticRegression predicts class probabilities using a sigmoid.\n\nQ: What is overfitting?\nA: A model learns noise in training data and performs poorly on unseen data. High variance is a symptom.\n\nQ: How to reduce overfitting?\nA: Use regularization, simplify the model, get more data, cross-validation, or dropout in neural nets.\n\nQ: What is regularization?\nA: Adding penalty terms to loss function. L1 (Lasso) shrinks some coefficients to zero, L2 (Ridge) shrinks them smoothly.\n\nQ: Purpose of RandomForest?\nA: An ensemble of decision trees trained with bootstrapped samples and feature randomness, improving accuracy and reducing variance.\n\nQ: What is DBSCAN?\nA: Density-Based Spatial Clustering groups points into clusters of high density and marks outliers as noise.\n\nQ: How is classification performance evaluated?\nA: Metrics: accuracy, precision, recall, F1, ROC-AUC, confusion matrix.\n\nQ: What is ROC-AUC?", "mimetype": "text/plain", "start_char_idx": 1855, "end_char_idx": 3977, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "e5c71aa9-0df5-407a-9568-8a81757ad8a3": {"__data__": {"id_": "e5c71aa9-0df5-407a-9568-8a81757ad8a3", "embedding": null, "metadata": {"file_path": "/Users/hemagunasekaran/Desktop/DATA SCIENCE & AI/rag_project/data/scikit_learn_notes.txt", "file_name": "scikit_learn_notes.txt", "file_type": "text/plain", "file_size": 8338, "creation_date": "2025-09-29", "last_modified_date": "2025-09-29"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "93b3519e-edc7-4b30-a0f7-74325161e4a0", "node_type": "4", "metadata": {"file_path": "/Users/hemagunasekaran/Desktop/DATA SCIENCE & AI/rag_project/data/scikit_learn_notes.txt", "file_name": "scikit_learn_notes.txt", "file_type": "text/plain", "file_size": 8338, "creation_date": "2025-09-29", "last_modified_date": "2025-09-29"}, "hash": "0adec09e4300379c83ef41d5f401a97485cf929744f050a6cbfec32db8458638", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "40874622-2ab8-4199-98ba-fd837ac51a75", "node_type": "1", "metadata": {"file_path": "/Users/hemagunasekaran/Desktop/DATA SCIENCE & AI/rag_project/data/scikit_learn_notes.txt", "file_name": "scikit_learn_notes.txt", "file_type": "text/plain", "file_size": 8338, "creation_date": "2025-09-29", "last_modified_date": "2025-09-29"}, "hash": "f4cbc3045f8e71c91ce20fc2279bcc1cd54876dbc6ac1f7d095e6df20a0084e6", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "68cad23f-3a43-41d6-bbe9-4fb44897adce", "node_type": "1", "metadata": {}, "hash": "a6debb1c59619fc8a79b6af026842d9466f2aa66c91df4fc4edadbad8c615142", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Q: What is DBSCAN?\nA: Density-Based Spatial Clustering groups points into clusters of high density and marks outliers as noise.\n\nQ: How is classification performance evaluated?\nA: Metrics: accuracy, precision, recall, F1, ROC-AUC, confusion matrix.\n\nQ: What is ROC-AUC?\nA: A measure of classifier ability to distinguish classes across thresholds. AUC closer to 1 = better.\n\nQ: What is GridSearchCV?\nA: Exhaustive search for best hyperparameters using cross-validation.\n\nQ: What is RandomizedSearchCV?\nA: Randomly samples hyperparameters to find good ones faster than grid search.\n\nQ: Purpose of stratified sampling?\nA: Ensures train/test splits maintain class distribution, avoiding skewed performance.\n\nQ: Feature importance in tree-based models?\nA: Measures how much each feature reduces impurity. High importance = strong influence on decisions.\n\nQ: What is feature selection and why?\nA: Selecting most relevant features to improve model performance and reduce overfitting.\n\nQ: How does DecisionTreeClassifier work?\nA: Splits data recursively based on features that maximize information gain or Gini reduction.\n\nQ: Difference between Bagging and Boosting?\nA: Bagging trains models independently on bootstrapped samples (RandomForest). Boosting trains sequentially, each correcting previous errors (AdaBoost, XGBoost).\n\nQ: What is Gradient Boosting?\nA: An ensemble technique where models are trained sequentially to minimize residual errors.\n\nQ: Difference between Ridge and Lasso?\nA: Ridge (L2) shrinks coefficients smoothly. Lasso (L1) shrinks some to zero, performing feature selection.\n\nQ: When to use ElasticNet?\nA: When combining both Ridge and Lasso penalties for correlated features and sparsity.\n\nQ: What is polynomial regression?\nA: Extends linear regression by adding polynomial features, useful for nonlinear relationships.\n\nQ: What is silhouette score?\nA: A clustering metric measuring how well a point fits within its cluster compared to others. Range -1 to 1.\n\nQ: What is adjusted Rand index?\nA: A clustering similarity measure between predicted and true labels, adjusted for chance.\n\nQ: What is SVM and kernel trick?\nA: Support Vector Machine finds max-margin hyperplane. Kernel trick maps data to higher dimensions for non-linear separation.", "mimetype": "text/plain", "start_char_idx": 3708, "end_char_idx": 5968, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "68cad23f-3a43-41d6-bbe9-4fb44897adce": {"__data__": {"id_": "68cad23f-3a43-41d6-bbe9-4fb44897adce", "embedding": null, "metadata": {"file_path": "/Users/hemagunasekaran/Desktop/DATA SCIENCE & AI/rag_project/data/scikit_learn_notes.txt", "file_name": "scikit_learn_notes.txt", "file_type": "text/plain", "file_size": 8338, "creation_date": "2025-09-29", "last_modified_date": "2025-09-29"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "93b3519e-edc7-4b30-a0f7-74325161e4a0", "node_type": "4", "metadata": {"file_path": "/Users/hemagunasekaran/Desktop/DATA SCIENCE & AI/rag_project/data/scikit_learn_notes.txt", "file_name": "scikit_learn_notes.txt", "file_type": "text/plain", "file_size": 8338, "creation_date": "2025-09-29", "last_modified_date": "2025-09-29"}, "hash": "0adec09e4300379c83ef41d5f401a97485cf929744f050a6cbfec32db8458638", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "e5c71aa9-0df5-407a-9568-8a81757ad8a3", "node_type": "1", "metadata": {"file_path": "/Users/hemagunasekaran/Desktop/DATA SCIENCE & AI/rag_project/data/scikit_learn_notes.txt", "file_name": "scikit_learn_notes.txt", "file_type": "text/plain", "file_size": 8338, "creation_date": "2025-09-29", "last_modified_date": "2025-09-29"}, "hash": "8623215397082b722e482b7833bfeeb42de909780dfc410ec90e42ec77d3ef98", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "12a1ff97-e515-4c1c-9e15-7e211bb44772", "node_type": "1", "metadata": {}, "hash": "06f26b84818b54f5e52230713e02e180e22705d823dadfad833971611d215f26", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Range -1 to 1.\n\nQ: What is adjusted Rand index?\nA: A clustering similarity measure between predicted and true labels, adjusted for chance.\n\nQ: What is SVM and kernel trick?\nA: Support Vector Machine finds max-margin hyperplane. Kernel trick maps data to higher dimensions for non-linear separation.\n\nQ: Difference between hard margin and soft margin SVM?\nA: Hard margin assumes perfect separability. Soft margin allows misclassifications with penalty for better generalization.\n\nQ: What is Naive Bayes?\nA: A probabilistic classifier using Bayes\u2019 theorem with feature independence assumption.\n\nQ: What is the curse of dimensionality?\nA: As features increase, data becomes sparse and distance-based methods (KNN) degrade in performance.\n\nQ: What is feature scaling and why important?\nA: Scaling ensures features contribute equally to distance-based algorithms (KNN, SVM, PCA).\n\nQ: What is imbalanced data problem?\nA: When one class dominates, accuracy becomes misleading. Use balanced metrics (F1, ROC-AUC).\n\nQ: How to handle imbalanced data?\nA: Use oversampling (SMOTE), undersampling, or class-weighted models.\n\nQ: Difference between classification and regression metrics?\nA: Classification uses accuracy, F1, ROC-AUC. Regression uses RMSE, MAE, R\u00b2.\n\nQ: What is one-hot encoding?\nA: Converts categorical variables into binary columns (dummy variables) for ML models.\n\nQ: Label encoding vs One-hot encoding?\nA: Label encoding assigns integers but may imply order. One-hot creates separate binary features.\n\nQ: What is a confusion matrix?\nA: A 2x2 (or larger) table summarizing TP, TN, FP, FN in classification.\n\nQ: Define precision and recall.\nA: Precision = TP / (TP+FP). Recall = TP / (TP+FN). Precision focuses on correctness, recall on completeness.\n\nQ: When to prefer precision vs recall?\nA: Precision: when false positives are costly (spam filter). Recall: when false negatives are costly (disease detection).\n\nQ: What is F1 score?\nA: Harmonic mean of precision and recall. Useful when classes are imbalanced.\n\nQ: What is r2_score?\nA: Regression metric showing proportion of variance explained by model.", "mimetype": "text/plain", "start_char_idx": 5670, "end_char_idx": 7778, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "12a1ff97-e515-4c1c-9e15-7e211bb44772": {"__data__": {"id_": "12a1ff97-e515-4c1c-9e15-7e211bb44772", "embedding": null, "metadata": {"file_path": "/Users/hemagunasekaran/Desktop/DATA SCIENCE & AI/rag_project/data/scikit_learn_notes.txt", "file_name": "scikit_learn_notes.txt", "file_type": "text/plain", "file_size": 8338, "creation_date": "2025-09-29", "last_modified_date": "2025-09-29"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "93b3519e-edc7-4b30-a0f7-74325161e4a0", "node_type": "4", "metadata": {"file_path": "/Users/hemagunasekaran/Desktop/DATA SCIENCE & AI/rag_project/data/scikit_learn_notes.txt", "file_name": "scikit_learn_notes.txt", "file_type": "text/plain", "file_size": 8338, "creation_date": "2025-09-29", "last_modified_date": "2025-09-29"}, "hash": "0adec09e4300379c83ef41d5f401a97485cf929744f050a6cbfec32db8458638", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "68cad23f-3a43-41d6-bbe9-4fb44897adce", "node_type": "1", "metadata": {"file_path": "/Users/hemagunasekaran/Desktop/DATA SCIENCE & AI/rag_project/data/scikit_learn_notes.txt", "file_name": "scikit_learn_notes.txt", "file_type": "text/plain", "file_size": 8338, "creation_date": "2025-09-29", "last_modified_date": "2025-09-29"}, "hash": "a18565ac4dbbd0f4fd651e9669d50e6475025957534a11bacc50b0a4ccc6fa4a", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "A: Precision: when false positives are costly (spam filter). Recall: when false negatives are costly (disease detection).\n\nQ: What is F1 score?\nA: Harmonic mean of precision and recall. Useful when classes are imbalanced.\n\nQ: What is r2_score?\nA: Regression metric showing proportion of variance explained by model. 1 = perfect fit, 0 = mean predictor.\n\nQ: What is mean_squared_error?\nA: Regression metric = average squared difference between predictions and actuals.\n\nQ: What is pipeline benefit?\nA: Ensures preprocessing is applied consistently to train/test and avoids data leakage.\n\nQ: What is data leakage?\nA: When test data information unintentionally influences training, leading to overoptimistic performance.\n\nQ: What is early stopping?\nA: Halting training when validation performance stops improving to prevent overfitting.", "mimetype": "text/plain", "start_char_idx": 7463, "end_char_idx": 8296, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}}}